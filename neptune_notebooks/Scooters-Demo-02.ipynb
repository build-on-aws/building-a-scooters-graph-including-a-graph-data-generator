{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e02637a9-23e8-4d4c-8a1e-53b38766228f",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=https://d1.awsstatic.com/logos/aws-logo-lockups/poweredbyaws/PB_AWS_logo_RGB_REV_SQ.8c88ac215fe4e441dc42865dd6962ed4f444a90d.png width=\"350\" class=\"center\">\n",
    "</div>\n",
    "<center> Welcome to the Scooters Graph Demo! </center>\n",
    "<center><b> YouTube channel: </b> <a href=\"https://www.youtube.com/@awsdevelopers\">AWS Developers</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56d5684",
   "metadata": {},
   "source": [
    "## Use LangChain Generative AI, using the Neptune Open Cypher QA Chain\n",
    "\n",
    "[LangChain](https://python.langchain.com/docs/get_started/introduction) is a framework for developing applications powered by language models. It enables applications that:\n",
    "\n",
    "* Are context-aware: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.)\n",
    "* Reason: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.)\n",
    "* The QA chain we use here, called NeptuneOpenCypherQAChain, queries Neptune graph databases using openCypher, returning human readable responses.\n",
    "\n",
    "See more at [LangChain docs](https://python.langchain.com/docs/use_cases/graph/neptune_cypher_qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad982486",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=https://d1.awsstatic.com/Neptune%20LangChain%20Integration.0537f77f5a3ed426c7f7aed220e6bd5e7b3ce57f.png width=\"650\" class=\"center\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e189d8-f29c-4872-9f66-80b6e21088bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%graph_notebook_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f47be99-22bd-4497-a79b-3732fbcfa68f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%graph_notebook_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1117d9-45cf-48e6-beb8-076e27a4024c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22483a1-d06a-4d3e-b154-9548a20458ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests, json, traceback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75db69e",
   "metadata": {},
   "source": [
    "<b> Requirement: </b><br>\n",
    "In the next cell, change the parameter values to those from the CDK deployment output or from the AWS console:\n",
    "1. Open the AWS console, go to CloudFormation and click on our CDK stack called \"ScootersNeptuneStack\".\n",
    "2. Click on the second tab, called \"Outputs\".\n",
    "3. Copy the value of outputneptuneendpoint and replace this to the value of NEPTUNE_SERVER_ENDPOINT\n",
    "4. Copy the value of outputneptuneiamrole and replace this to the value of IAM_ROLE_ARN\n",
    "5. Copy the value of outputs3bucket and replace this to the value of S3_BUCKET\n",
    "6. For AWS_REGION, enter the AWS region where you're doing all this demo; e.g. eu-west-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510a2448-93be-46ae-ad76-be0c278a3c9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Â Input Neptune parameters, to load the generated data:\n",
    "NEPTUNE_SERVER_ENDPOINT = 'change_me'\n",
    "IAM_ROLE_ARN = 'change_me'\n",
    "S3_BUCKET = 'change_me'\n",
    "AWS_REGION = 'change_me'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42caecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neptune endpoint. You don't need to change anything here, unless you changed the Neptune's database port.\n",
    "PORT = 8182\n",
    "ENDPOINT = f\"https://{NEPTUNE_SERVER_ENDPOINT}:{PORT}/loader\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6a9e6f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a483e66",
   "metadata": {},
   "source": [
    "### Let's now Use LangChain Generative AI, through the Neptune Open Cypher QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e63d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain_community langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7de2174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.graphs import NeptuneGraph\n",
    "from langchain.chains import NeptuneOpenCypherQAChain\n",
    "from langchain.llms.bedrock import Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9d5d7c",
   "metadata": {},
   "source": [
    "### Create the helper function to get inference parameters based on the model.\n",
    "\n",
    "> Source / credits of this function: [Building with Amazon Bedrock and LangChain](https://catalog.workshops.aws/building-with-amazon-bedrock/en-US/foundation/bedrock-inference-parameters)\n",
    "\n",
    "- Each model provider has its own set of inference parameters. This function returns a set of custom parameters based on the first portion of each model's id.\n",
    "\n",
    "- You can experiment with different inference parameters using the Text playground in the Bedrock console. You can use the View API request button in the playground to get the parameters to use in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4511928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inference_parameters(model):\n",
    "    \"\"\"\n",
    "    Return a default set of parameters based on the model's provider\n",
    "    \"\"\"\n",
    "    bedrock_model_provider = model.split('.')[0] #grab the model provider from the first part of the model id\n",
    "    \n",
    "    if (bedrock_model_provider == 'anthropic'): #Anthropic model\n",
    "        return { #anthropic\n",
    "            \"max_tokens_to_sample\": 512,\n",
    "            \"temperature\": 0, \n",
    "            \"top_k\": 250, \n",
    "            \"top_p\": 1, \n",
    "            \"stop_sequences\": [\"\\n\\nHuman:\"] \n",
    "           }\n",
    "    \n",
    "    elif (bedrock_model_provider == 'ai21'): #AI21\n",
    "        return { #AI21\n",
    "            \"maxTokens\": 512, \n",
    "            \"temperature\": 0, \n",
    "            \"topP\": 0.5, \n",
    "            \"stopSequences\": [], \n",
    "            \"countPenalty\": {\"scale\": 0 }, \n",
    "            \"presencePenalty\": {\"scale\": 0 }, \n",
    "            \"frequencyPenalty\": {\"scale\": 0 } \n",
    "           }\n",
    "    \n",
    "    elif (bedrock_model_provider == 'cohere'): #COHERE\n",
    "        return {\n",
    "            \"max_tokens\": 512,\n",
    "            \"temperature\": 0,\n",
    "            \"p\": 0.01,\n",
    "            \"k\": 0,\n",
    "            \"stop_sequences\": [],\n",
    "            \"return_likelihoods\": \"NONE\"\n",
    "        }\n",
    "    \n",
    "    elif (bedrock_model_provider == 'meta'): #META\n",
    "        return {\n",
    "            \"temperature\": 0,\n",
    "            \"top_p\": 0.9,\n",
    "            \"max_gen_len\": 512\n",
    "        }\n",
    "\n",
    "    else: #Amazon\n",
    "        #For the LangChain Bedrock implementation, these parameters will be added to the \n",
    "        #textGenerationConfig item that LangChain creates for us\n",
    "        return { \n",
    "            \"maxTokenCount\": 512, \n",
    "            \"stopSequences\": [], \n",
    "            \"temperature\": 0, \n",
    "            \"topP\": 0.9 \n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04013fbd",
   "metadata": {},
   "source": [
    "#### Let's now query our graph, using natural language via LangChain\n",
    "<b> IMPORTANT: </b> Don't forget [to grant your account](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html) with LLM model access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e0582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection:\n",
    "host = NEPTUNE_SERVER_ENDPOINT\n",
    "port = PORT\n",
    "use_https = True\n",
    "\n",
    "# Get model inference parameters. Feel free to change the model.\n",
    "model = 'anthropic.claude-v2:1'\n",
    "model_kwargs = get_inference_parameters(model)\n",
    "\n",
    "# Model setup\n",
    "graph = NeptuneGraph(host=host, port=port, use_https=use_https)\n",
    "llm = Bedrock(\n",
    "    region_name=\"us-west-2\",\n",
    "    model_id=model,\n",
    "    model_kwargs = model_kwargs\n",
    ")\n",
    "\n",
    "# Let's use NL to ask our LLM to traverse the graph\n",
    "chain = NeptuneOpenCypherQAChain.from_llm(llm=llm, graph=graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3c98ae",
   "metadata": {},
   "source": [
    "### Let's interrogate our graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f3f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke(\"how many scooters do I have?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c77dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt_template=\"\"\"\n",
    "    Human: Please, respond the question directly. The data model for our database graph follows the following guidelines:\n",
    "    - All scooters are labelled as scooter.\n",
    "    - All the labels for the edges is 'has'\n",
    "    - Entering a dash symbol \"-\" as the edge, the labels for the nodes are connected in the following way:\n",
    "    \n",
    "    scooter - incident - legal_case\n",
    "    \n",
    "    question: how many scooters do I have that have had an incident?\n",
    "\n",
    "    Assistant:\"\"\"\n",
    "\n",
    "# Let's now ask a more-complex question:\n",
    "chain.invoke(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19bdcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "\n",
    "// Confirm the LLM output. This query will count connected Scooters that had an incident.\n",
    "g.V().hasLabel('scooter').repeat(__.out().simplePath()).until(hasLabel('incident')).out().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b71829",
   "metadata": {},
   "source": [
    "### Let's now see what is LangChain executing internally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceeb2868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's modify the LLM config parameters\n",
    "chain = NeptuneOpenCypherQAChain.from_llm(llm=llm, graph=graph, verbose=True, top_k=10, return_direct=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d9742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count scooters and show OpenCypher internal query executed by LLM\n",
    "chain.invoke(\"\"\"\n",
    "    Human: Please, respond the question directly. The data model for our database graph follows the following guidelines:\n",
    "    - All scooters are labelled as scooter.\n",
    "    - All the labels for the edges is 'has'\n",
    "    - Entering a dash symbol \"-\" as the edge, the labels for the nodes are connected in the following way:\n",
    "    \n",
    "    scooter - incident - legal_case\n",
    "    \n",
    "    question: how many scooters do I have?\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb0ce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt_template=\"\"\"\n",
    "    Human: Please, respond the question directly. The data model for our database graph follows the following guidelines:\n",
    "    - All scooters are labelled as scooter.\n",
    "    - All the labels for the edges is 'has'\n",
    "    - All parts are labelled as part_<<something>>, where <<something>> can be suspension, tyres (back_tyre or front_tyre), steering, etc.\n",
    "      For example, for a tyre the label will be part_tyre or for a suspension the label will be part_suspension. \n",
    "    - Entering a dash symbol \"-\" as the edge, the labels for the nodes are connected in the following way:\n",
    "    \n",
    "    scooter - part - fault - claim\n",
    "    \n",
    "    question: how many suspension parts do I have that have any fault?\n",
    "\n",
    "    Assistant:\"\"\"\n",
    "\n",
    "# Let's now ask a more-complex question:\n",
    "chain.invoke(prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5e7360",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%gremlin\n",
    "\n",
    "// Confirm the LLM output. This query will count connected Scooters that have faulty suspensions\n",
    "g.V().hasLabel('part_suspension').repeat(__.out().simplePath()).until(hasLabel('fault')).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b62bcd",
   "metadata": {},
   "source": [
    "## Thanks!\n",
    "\n",
    "If you want to learn about Simplifying Graph Queries With LLMs and LangChain, I recommend you to visit [this video from our YouTube channel](https://www.youtube.com/watch?v=B7GtC1IeIUA). In here, Kelvin Lawrence, graph architect and author of the fantastic [online book \"Practical Gremlin\"](http://kelvinlawrence.net/book/Gremlin-Graph-Guide.html), will dive deep into this topic using a popular and public Airports graph dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
